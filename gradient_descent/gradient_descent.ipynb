{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Lab_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLde1OhxYuGL"
      },
      "source": [
        "#drive.mount(\"/content/drive\", force_remount=True)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3LQi4ZtPkY5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn import datasets\n",
        "\n",
        "data = datasets.load_boston(return_X_y=False)\n",
        "boston = datasets.load_boston(return_X_y=False)\n",
        "# data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "slXDN1YTPuiq",
        "outputId": "d0029998-a9d1-496f-9df6-a4c398528170"
      },
      "source": [
        "import pandas as pd\n",
        "# creating the dataframe\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "dataframe = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df.shape\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCFNU2MkP3ZH",
        "outputId": "7f7cfe84-cd60-4e49-b82c-adff3ebd6814"
      },
      "source": [
        "a = 0.3 \n",
        "dataset1 = dataframe.copy()\n",
        "train_dataset = dataset1.sample(frac = a)\n",
        "dataset1 = dataset1.drop(train_dataset.index)\n",
        "a = 1/7\n",
        "validation_dataset = dataset1.sample(frac = a)\n",
        "test_dataset = dataset1.drop(validation_dataset.index)\n",
        "\n",
        "print(\"train_dataset.....\")\n",
        "print(train_dataset)\n",
        "\n",
        "print(\"validation_dataset.....\")\n",
        "print(validation_dataset)\n",
        "\n",
        "print(\"test_dataset.....\")\n",
        "print(test_dataset)\n",
        "\n",
        "X = boston.data                            \n",
        "Y = boston.target \n",
        "# print(Y)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset.....\n",
            "         CRIM    ZN  INDUS  CHAS    NOX  ...   RAD    TAX  PTRATIO       B  LSTAT\n",
            "125   0.16902   0.0  25.65   0.0  0.581  ...   2.0  188.0     19.1  385.02  14.81\n",
            "487   4.83567   0.0  18.10   0.0  0.583  ...  24.0  666.0     20.2  388.22  11.45\n",
            "494   0.27957   0.0   9.69   0.0  0.585  ...   6.0  391.0     19.2  396.90  13.59\n",
            "216   0.04560   0.0  13.89   1.0  0.550  ...   5.0  276.0     16.4  392.80  13.51\n",
            "29    1.00245   0.0   8.14   0.0  0.538  ...   4.0  307.0     21.0  380.23  11.98\n",
            "..        ...   ...    ...   ...    ...  ...   ...    ...      ...     ...    ...\n",
            "305   0.05479  33.0   2.18   0.0  0.472  ...   7.0  222.0     18.4  393.36   8.93\n",
            "470   4.34879   0.0  18.10   0.0  0.580  ...  24.0  666.0     20.2  396.90  16.29\n",
            "410  51.13580   0.0  18.10   0.0  0.597  ...  24.0  666.0     20.2    2.60  10.11\n",
            "335   0.03961   0.0   5.19   0.0  0.515  ...   5.0  224.0     20.2  396.90   8.01\n",
            "333   0.05083   0.0   5.19   0.0  0.515  ...   5.0  224.0     20.2  389.71   5.68\n",
            "\n",
            "[152 rows x 13 columns]\n",
            "validation_dataset.....\n",
            "         CRIM    ZN  INDUS  CHAS     NOX  ...   RAD    TAX  PTRATIO       B  LSTAT\n",
            "441   9.72418   0.0  18.10   0.0  0.7400  ...  24.0  666.0     20.2  385.96  19.52\n",
            "321   0.18159   0.0   7.38   0.0  0.4930  ...   5.0  287.0     19.6  396.90   6.87\n",
            "426  12.24720   0.0  18.10   0.0  0.5840  ...  24.0  666.0     20.2   24.65  15.69\n",
            "347   0.01870  85.0   4.15   0.0  0.4290  ...   4.0  351.0     17.9  392.43   6.36\n",
            "153   2.14918   0.0  19.58   0.0  0.8710  ...   5.0  403.0     14.7  261.95  15.79\n",
            "481   5.70818   0.0  18.10   0.0  0.5320  ...  24.0  666.0     20.2  393.07   7.74\n",
            "219   0.11425   0.0  13.89   1.0  0.5500  ...   5.0  276.0     16.4  393.74  10.50\n",
            "467   4.42228   0.0  18.10   0.0  0.5840  ...  24.0  666.0     20.2  331.29  21.32\n",
            "245   0.19133  22.0   5.86   0.0  0.4310  ...   7.0  330.0     19.1  389.13  18.46\n",
            "485   3.67367   0.0  18.10   0.0  0.5830  ...  24.0  666.0     20.2  388.62  10.58\n",
            "384  20.08490   0.0  18.10   0.0  0.7000  ...  24.0  666.0     20.2  285.83  30.63\n",
            "353   0.01709  90.0   2.02   0.0  0.4100  ...   5.0  187.0     17.0  384.46   4.50\n",
            "373  11.10810   0.0  18.10   0.0  0.6680  ...  24.0  666.0     20.2  396.90  34.77\n",
            "118   0.13058   0.0  10.01   0.0  0.5470  ...   6.0  432.0     17.8  338.63  15.37\n",
            "208   0.13587   0.0  10.59   1.0  0.4890  ...   4.0  277.0     18.6  381.32  14.66\n",
            "238   0.08244  30.0   4.93   0.0  0.4280  ...   6.0  300.0     16.6  379.41   6.36\n",
            "114   0.14231   0.0  10.01   0.0  0.5470  ...   6.0  432.0     17.8  388.74  10.45\n",
            "285   0.01096  55.0   2.25   0.0  0.3890  ...   1.0  300.0     15.3  394.72   8.23\n",
            "489   0.18337   0.0  27.74   0.0  0.6090  ...   4.0  711.0     20.1  344.05  23.97\n",
            "275   0.09604  40.0   6.41   0.0  0.4470  ...   4.0  254.0     17.6  396.90   2.98\n",
            "457   8.20058   0.0  18.10   0.0  0.7130  ...  24.0  666.0     20.2    3.50  16.94\n",
            "378  23.64820   0.0  18.10   0.0  0.6710  ...  24.0  666.0     20.2  396.90  23.69\n",
            "99    0.06860   0.0   2.89   0.0  0.4450  ...   2.0  276.0     18.0  396.90   6.19\n",
            "35    0.06417   0.0   5.96   0.0  0.4990  ...   5.0  279.0     19.2  396.90   9.68\n",
            "327   0.24103   0.0   7.38   0.0  0.4930  ...   5.0  287.0     19.6  396.90  12.79\n",
            "137   0.35233   0.0  21.89   0.0  0.6240  ...   4.0  437.0     21.2  394.08  14.59\n",
            "477  15.02340   0.0  18.10   0.0  0.6140  ...  24.0  666.0     20.2  349.48  24.91\n",
            "248   0.16439  22.0   5.86   0.0  0.4310  ...   7.0  330.0     19.1  374.71   9.52\n",
            "453   8.24809   0.0  18.10   0.0  0.7130  ...  24.0  666.0     20.2  375.87  16.74\n",
            "70    0.08826   0.0  10.81   0.0  0.4130  ...   4.0  305.0     19.2  383.73   6.72\n",
            "322   0.35114   0.0   7.38   0.0  0.4930  ...   5.0  287.0     19.6  396.90   7.70\n",
            "313   0.26938   0.0   9.90   0.0  0.5440  ...   4.0  304.0     18.4  393.39   7.90\n",
            "190   0.09068  45.0   3.44   0.0  0.4370  ...   5.0  398.0     15.2  377.68   5.10\n",
            "292   0.03615  80.0   4.95   0.0  0.4110  ...   4.0  245.0     19.2  396.90   4.70\n",
            "60    0.14932  25.0   5.13   0.0  0.4530  ...   8.0  284.0     19.7  395.11  13.15\n",
            "433   5.58107   0.0  18.10   0.0  0.7130  ...  24.0  666.0     20.2  100.19  16.22\n",
            "431  10.06230   0.0  18.10   0.0  0.5840  ...  24.0  666.0     20.2   81.33  19.69\n",
            "257   0.61154  20.0   3.97   0.0  0.6470  ...   5.0  264.0     13.0  389.70   5.12\n",
            "490   0.20746   0.0  27.74   0.0  0.6090  ...   4.0  711.0     20.1  318.43  29.68\n",
            "109   0.26363   0.0   8.56   0.0  0.5200  ...   5.0  384.0     20.9  391.23  15.55\n",
            "400  25.04610   0.0  18.10   0.0  0.6930  ...  24.0  666.0     20.2  396.90  26.77\n",
            "337   0.03041   0.0   5.19   0.0  0.5150  ...   5.0  224.0     20.2  394.81  10.56\n",
            "500   0.22438   0.0   9.69   0.0  0.5850  ...   6.0  391.0     19.2  396.90  14.33\n",
            "131   1.19294   0.0  21.89   0.0  0.6240  ...   4.0  437.0     21.2  396.90  12.26\n",
            "334   0.03738   0.0   5.19   0.0  0.5150  ...   5.0  224.0     20.2  389.40   6.75\n",
            "493   0.17331   0.0   9.69   0.0  0.5850  ...   6.0  391.0     19.2  396.90  12.01\n",
            "281   0.03705  20.0   3.33   0.0  0.4429  ...   5.0  216.0     14.9  392.23   4.59\n",
            "460   4.81213   0.0  18.10   0.0  0.7130  ...  24.0  666.0     20.2  255.23  16.42\n",
            "254   0.04819  80.0   3.64   0.0  0.3920  ...   1.0  315.0     16.4  392.89   6.57\n",
            "459   6.80117   0.0  18.10   0.0  0.7130  ...  24.0  666.0     20.2  396.90  14.70\n",
            "210   0.17446   0.0  10.59   1.0  0.4890  ...   4.0  277.0     18.6  393.25  17.27\n",
            "\n",
            "[51 rows x 13 columns]\n",
            "test_dataset.....\n",
            "        CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
            "2    0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
            "3    0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
            "5    0.02985   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.12   5.21\n",
            "6    0.08829  12.5   7.87   0.0  0.524  ...  5.0  311.0     15.2  395.60  12.43\n",
            "8    0.21124  12.5   7.87   0.0  0.524  ...  5.0  311.0     15.2  386.63  29.93\n",
            "..       ...   ...    ...   ...    ...  ...  ...    ...      ...     ...    ...\n",
            "499  0.17783   0.0   9.69   0.0  0.585  ...  6.0  391.0     19.2  395.77  15.10\n",
            "501  0.06263   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  391.99   9.67\n",
            "503  0.06076   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   5.64\n",
            "504  0.10959   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  393.45   6.48\n",
            "505  0.04741   0.0  11.93   0.0  0.573  ...  1.0  273.0     21.0  396.90   7.88\n",
            "\n",
            "[303 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1bpXAGvv-n7"
      },
      "source": [
        "\n",
        "data = boston.data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "standardised_data = sc.fit_transform(data)\n",
        " \n",
        "additional_feature = np.ones(boston.data.shape[0])\n",
        "\n",
        "feature_data = np.vstack((additional_feature,standardised_data.T)).T\n",
        "\n",
        "# Actual prices of houses\n",
        "target_price = boston.target\n",
        "\n",
        "\n",
        "# Stochastic Gradient Descent Algorithm :\n",
        "# Let 'K' be the number of random rows selected out of the dataset\n",
        "# Initialize the weight vector\n",
        "#Let r = learning_rate and m = number of training_examples\n",
        "# Let r =1\n",
        "# repeat until convergence {\n",
        "#    weight[j] = weight[j] - (r/m)*((Σfrom i=1 to K)of(((weight.T * feature_data[i]) - target_price[i])* feature_data[i,j])\n",
        "#    r /= 2  \n",
        "#}\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(feature_data, target_price, test_size = 0.3, random_state = 5)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQx5V5nGv-qY",
        "outputId": "8faf6aaf-02f9-4d2c-80f5-b5cf62fb9b21"
      },
      "source": [
        "# Initialising weight vector\n",
        "# Generating 14 normally distributed values\n",
        "weights = np.random.normal(0,1,feature_data.shape[1])\n",
        "\n",
        "# Initialised Weights\n",
        "weights"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.8559244 , -0.73615475,  1.19051558, -0.39391712, -0.51711159,\n",
              "       -1.05672855, -0.3170854 , -1.49434358, -0.12622536,  1.54112609,\n",
              "        0.07922147,  0.07471363, -2.53533894,  3.55948403])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Hb84xA0Ly5"
      },
      "source": [
        "temp_w = np.zeros(feature_data.shape[1])\n",
        "from numpy import random\n",
        "epochs = [0.1, 0.01, 0.02, 0.03, 0.001]\n",
        "rho = [0.5, 0.6 , 0.7, 0.8, 0.9]\n",
        "learning_rate = [10, 20 , 30, 40, 50]\n",
        "\n",
        "def SGD(f, w0, alpha, num_iters):\n",
        "  # Initialised learning rate\n",
        "  r = 0.001  \n",
        "  iterations = 1000\n",
        "  # training samples\n",
        "  m = X_train.shape[0]    \n",
        "  # batch size is to get batches for Stochastic Gradient Descent\n",
        "  batch_size = 20 \n",
        "\t\n",
        "  random_ids = random.choice(m,m,replace=False)\n",
        "  X_shuffled = X_train[random_ids,:]\n",
        "  y_shuffled = Y_train[random_ids]\n",
        "  mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for i in range(0, m, batch_size)]\n",
        "  # stochastic gradient descent : SGD\n",
        "  while(iterations >=0):\n",
        "      for batch in mini_batches:\n",
        "          X_batch = batch[0]\n",
        "          Y_batch = batch[1]\n",
        "          for j in range(0,feature_data.shape[1]):\n",
        "              temp_sum = 0\n",
        "              for i in range(0,X_batch.shape[0]):\n",
        "                  temp_sum += (( (np.sum( sc.inverse_transform(weights[1:14] * X_batch[i,1:])) + weights[0]*X_batch[i,0]) - Y_batch[i]) * X_batch[i,j])\n",
        "              temp_w[j] = weights[j] - ((r/X_batch.shape[0])*temp_sum)\n",
        "          weights = temp_w\n",
        "      iterations -= 1 \n",
        "    \n",
        "# Weights of manual sgd\n",
        "  manual_sgd_weights = weights\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCKAWomqv-ss"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHbIG1Fcv-wD"
      },
      "source": [
        "# Now predicting the house prices on X_test data\n",
        "manual_sgd_predictions = np.zeros(X_test.shape[0])\n",
        "for itr in range(0,X_test.shape[0]):\n",
        "    manual_sgd_predictions[itr] = np.sum(sc.inverse_transform(weights[1:14]*X_test[itr,1:])) + weights[0]*X_test[itr,0]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO7YEjob0MEV"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XHvLQA50MHU"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc_kUF2Pi2oh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bcc021a-94af-4bd6-98f1-0cf77b5017a6"
      },
      "source": [
        "# Batch and stochastic both .\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "boston = datasets.load_boston(return_X_y=False)\n",
        "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "boston_df['PRICE'] = boston.target\n",
        "print(boston_df)\n",
        "\n",
        "train_dataset = boston_df.sample(frac=0.3)\n",
        "dataframe = boston_df.drop(train_dataset.index)\n",
        "validation_dataset = dataframe.sample(frac=(.1/.7))\n",
        "test_dataset = dataframe.drop(validation_dataset.index)\n",
        "\n",
        "print(\"train dataset\\n\", train_dataset)\n",
        "# print(\"validation dataset\\n\", validation_dataset)\n",
        "# print(\"test dataset\\n\", test_dataset)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def hypothesis(w, index, dataset): #h(x)\n",
        "  sample =dataset[index][:-1]\n",
        "  sample = np.concatenate([[1], sample])\n",
        "  array = np.multiply(w, sample)\n",
        "  return np.sum(array)\n",
        "\n",
        "\n",
        "w=[1,1,1]\n",
        "dataset = np.array([[1,2,6], [2,10,24]])\n",
        "\n",
        "print(hypothesis(w, 0, dataset))\n",
        "print(dataset)\n",
        "\n",
        "# to calculate the predicted value with the help of hypothesis function\n",
        "def predicted_values(w, dataset):\n",
        "  predicted = []\n",
        "  for i in range(len(dataset)):\n",
        "    predicted.append(hypothesis(w, i, dataset))\n",
        "  \n",
        "  return np.array(predicted)\n",
        "\n",
        "predicted = predicted_values(w, dataset)\n",
        "\n",
        "print(predicted)\n",
        "\n",
        "\n",
        "# to calculate mean square error\n",
        "def MSE(predicted, actual):\n",
        "  return np.square(np.subtract(predicted, actual)).mean()/2\n",
        "\n",
        "MSE(predicted, dataset[:,-1])\n",
        "\n",
        "# to update w using batch gradient decendent\n",
        "def update_parameter(w, alpha, dataset):\n",
        "  predicted = predicted_values(w, dataset)\n",
        "  m = len(dataset)\n",
        "  w[0] = w[0] - (alpha /m)*(np.sum(np.subtract(predicted, dataset[:, -1])))\n",
        "  for i in range(1, len(w)):\n",
        "    w[i] = w[i] - (alpha /m)*(np.sum(np.multiply(np.subtract(predicted, dataset[:, -1]), dataset[:, i-1])))\n",
        "  return w\n",
        "\n",
        "\n",
        "#w=update_parameter(w, 0.1, dataset)\n",
        "print(update_parameter(w, 0.1, dataset))\n",
        "\n",
        "# to update w using stochastic gradient decendent\n",
        "def update_parameter_stochastic(w, alpha, dataset):\n",
        "  np.random.shuffle(dataset)\n",
        "  m = len(dataset)\n",
        "  for j in range(m):\n",
        "    hy = hypothesis(w, j, dataset)\n",
        "    w[0] = w[0] - (alpha /m)*(hy- dataset[:, -1][j])\n",
        "    for i in range(1, len(w)):\n",
        "      w[i] = w[i] - (alpha /m)*((hy- dataset[:, -1][j])* dataset[:, i-1][j])\n",
        "  return w\n",
        "print(update_parameter_stochastic([1,1,1], 0.1, dataset))\n",
        "\n",
        "\n",
        "def linear_regression(epoch, alpha, rho, w, train_dataset):\n",
        "  epochs_MSE=[]\n",
        "  pre_MSE = 0\n",
        "  curr_MSE = 0\n",
        "  for itr in range(epoch):\n",
        "    predicted = predicted_values(w, train_dataset)\n",
        "    curr_MSE = MSE(predicted, train_dataset[:,-1])\n",
        "    w = update_parameter_stochastic(w, alpha, train_dataset)\n",
        "\n",
        "    epochs_MSE.append(curr_MSE)\n",
        "\n",
        "    if abs(curr_MSE - pre_MSE) <= rho:\n",
        "      break;\n",
        "  return w, epochs_MSE\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "w=[1 for _ in range(14)]\n",
        "\n",
        "normalized = preprocessing.normalize(train_dataset.to_numpy())\n",
        "w1, trains_MSE1 = linear_regression(10, 0.1, 0.5, w, normalized)\n",
        "\n",
        "\n",
        "validation = preprocessing.normalize(validation_dataset.to_numpy())\n",
        "predicted = predicted_values(w1, validation)\n",
        "val_MSE1 = MSE(predicted, validation[:,-1])\n",
        "\n",
        "# print(w1)\n",
        "# print(trains_MSE1)\n",
        "print(\"MSE of validation set\")\n",
        "print(val_MSE1)\n",
        "\n",
        "\n",
        "\n",
        "w=[1 for _ in range(14)]\n",
        "w2, trains_MSE2 = linear_regression(20, 0.01, 0.6, w, normalized)\n",
        "\n",
        "predicted = predicted_values(w2, validation)\n",
        "val_MSE2 = MSE(predicted, validation[:,-1])\n",
        "\n",
        "# print(w2)\n",
        "# print(trains_MSE2)\n",
        "print(\"MSE of validation set\")\n",
        "print(val_MSE2)\n",
        "\n",
        "\n",
        "\n",
        "w=[1 for _ in range(14)]\n",
        "w3, trains_MSE3 = linear_regression(30, 0.02, 0.7, w, normalized)\n",
        "\n",
        "predicted = predicted_values(w3, validation)\n",
        "val_MSE3 = MSE(predicted, validation[:,-1])\n",
        "\n",
        "# print(w3)\n",
        "# print(trains_MSE3)\n",
        "print(\"MSE of validation set\")\n",
        "print(val_MSE3)\n",
        "\n",
        "\n",
        "w=[1 for _ in range(14)]\n",
        "w4, trains_MSE4 = linear_regression(40, 0.03, 0.8, w, normalized)\n",
        "\n",
        "predicted = predicted_values(w4, validation)\n",
        "val_MSE4 = MSE(predicted, validation[:,-1])\n",
        "\n",
        "# print(w4)\n",
        "# print(trains_MSE4)\n",
        "print(\"MSE of validation set\")\n",
        "print(val_MSE4)\n",
        "\n",
        "w=[1 for _ in range(14)]\n",
        "w5, trains_MSE5 = linear_regression(50, 0.001, 0.9, w, normalized)\n",
        "\n",
        "predicted = predicted_values(w5, validation)\n",
        "val_MSE5 = MSE(predicted, validation[:,-1])\n",
        "\n",
        "# print(w5)\n",
        "# print(trains_MSE5)\n",
        "print(\"MSE of validation set\")\n",
        "print(val_MSE5)\n",
        "\n",
        "\n",
        "print(\"All MSE considering the best hyperparameter\")\n",
        "print(\"Validation MSE :\", val_MSE1)\n",
        "\n",
        "\n",
        "train = preprocessing.normalize(train_dataset.to_numpy())\n",
        "predicted = predicted_values(w1, train)\n",
        "best_trainMSE = MSE(predicted, train[:,-1])\n",
        "\n",
        "print(\"Train MSE:\", best_trainMSE)\n",
        "\n",
        "test = preprocessing.normalize(test_dataset.to_numpy())\n",
        "predicted = predicted_values(w1, test)\n",
        "best_testMSE = MSE(predicted, test[:,-1])\n",
        "\n",
        "print(\"TEST MSE:\", best_testMSE)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# print(trains_MSE1)\n",
        "epoch = [i for i in range(len(trains_MSE1))]\n",
        "plt.plot(epoch, trains_MSE1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  PRICE\n",
            "0    0.00632  18.0   2.31   0.0  0.538  ...  296.0     15.3  396.90   4.98   24.0\n",
            "1    0.02731   0.0   7.07   0.0  0.469  ...  242.0     17.8  396.90   9.14   21.6\n",
            "2    0.02729   0.0   7.07   0.0  0.469  ...  242.0     17.8  392.83   4.03   34.7\n",
            "3    0.03237   0.0   2.18   0.0  0.458  ...  222.0     18.7  394.63   2.94   33.4\n",
            "4    0.06905   0.0   2.18   0.0  0.458  ...  222.0     18.7  396.90   5.33   36.2\n",
            "..       ...   ...    ...   ...    ...  ...    ...      ...     ...    ...    ...\n",
            "501  0.06263   0.0  11.93   0.0  0.573  ...  273.0     21.0  391.99   9.67   22.4\n",
            "502  0.04527   0.0  11.93   0.0  0.573  ...  273.0     21.0  396.90   9.08   20.6\n",
            "503  0.06076   0.0  11.93   0.0  0.573  ...  273.0     21.0  396.90   5.64   23.9\n",
            "504  0.10959   0.0  11.93   0.0  0.573  ...  273.0     21.0  393.45   6.48   22.0\n",
            "505  0.04741   0.0  11.93   0.0  0.573  ...  273.0     21.0  396.90   7.88   11.9\n",
            "\n",
            "[506 rows x 14 columns]\n",
            "train dataset\n",
            "          CRIM    ZN  INDUS  CHAS    NOX  ...    TAX  PTRATIO       B  LSTAT  PRICE\n",
            "202   0.02177  82.5   2.03   0.0  0.415  ...  348.0     14.7  395.38   3.11   42.3\n",
            "345   0.03113   0.0   4.39   0.0  0.442  ...  352.0     18.8  385.64  10.53   17.5\n",
            "433   5.58107   0.0  18.10   0.0  0.713  ...  666.0     20.2  100.19  16.22   14.3\n",
            "440  22.05110   0.0  18.10   0.0  0.740  ...  666.0     20.2  391.45  22.11   10.5\n",
            "213   0.14052   0.0  10.59   0.0  0.489  ...  277.0     18.6  385.81   9.38   28.1\n",
            "..        ...   ...    ...   ...    ...  ...    ...      ...     ...    ...    ...\n",
            "16    1.05393   0.0   8.14   0.0  0.538  ...  307.0     21.0  386.85   6.58   23.1\n",
            "410  51.13580   0.0  18.10   0.0  0.597  ...  666.0     20.2    2.60  10.11   15.0\n",
            "59    0.10328  25.0   5.13   0.0  0.453  ...  284.0     19.7  396.90   9.22   19.6\n",
            "93    0.02875  28.0  15.04   0.0  0.464  ...  270.0     18.2  396.33   6.21   25.0\n",
            "499   0.17783   0.0   9.69   0.0  0.585  ...  391.0     19.2  395.77  15.10   17.5\n",
            "\n",
            "[152 rows x 14 columns]\n",
            "4\n",
            "[[ 1  2  6]\n",
            " [ 2 10 24]]\n",
            "[ 4 13]\n",
            "[1.65, 2.2, 6.7]\n",
            "[1.5350000000000001, 1.9700000000000002, 5.550000000000001]\n",
            "MSE of validation set\n",
            "0.3136192252334964\n",
            "MSE of validation set\n",
            "1.498986957487471\n",
            "MSE of validation set\n",
            "0.5876286563487332\n",
            "MSE of validation set\n",
            "0.6353655444626344\n",
            "MSE of validation set\n",
            "2.686534541857061\n",
            "All MSE considering the best hyperparameter\n",
            "Validation MSE : 0.3136192252334964\n",
            "Train MSE: 0.3263195177903169\n",
            "TEST MSE: 0.3218266807604888\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f3e1c9ff750>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3ozRjbVdsXt"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnwpZCl5ktiW"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}